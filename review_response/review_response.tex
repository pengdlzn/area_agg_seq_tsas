% Copyright Javier Sánchez-Monedero.
% Please report bugs and suggestions to (jsanchezm at uco.es)
%
% This document is released under a Creative Commons Licence 
% CC-BY-SA (http://creativecommons.org/licenses/by-sa/3.0/) 
%
% BASIC INSTRUCTIONS: 
% 1. Load and set up proper language packages
% 2. Complete the paper data commands
% 3. Use commands \rcomment and \newtext as shown in the example

\documentclass[a4paper,twoside,11pt]{reviewresponse}
\input{review_response_setting}


\begin{document}
	
\thispagestyle{plain}

\begin{center}
	{\Large\myTitle} \vspace{0.5cm} \\
%	{\large\myJournal} \vspace{0.5cm} \\
	\today \vspace{0.5cm} \\
	\myAuthors \\
	\url{\myEmail} \vspace{1cm} \\
	\myDept
\end{center}

\tableofcontents

\begin{abstract}
We thank the anonymous reviewers for their comments.
These comments were very helpful for us to improve our paper. 
In this document, 
we provide detailed responses to each of the reviewers' comments. 
The following list summarizes our most important changes:
\begin{itemize}
    \item  An NP-hardness proof has been added.
    \item  The illustration of the exponential lower bound has been improved.
    \item  The approach based on integer linear programming 
    has been moved to the appendix.
    \item  Several comments concerning the applied compactness measures 
    have been incorporated.
\end{itemize}
\end{abstract}

\section{Reviewer 1}

\rcommentnoskip{
The paper describes an interpretation of the area aggregation problem as the computation of a shortest path in a suitably defined graph.
The contents of the paper are an interesting read and certainly well within the scope of TSAS.

Though I have some comments/questions about certain parts (see below), the paper is of good quality already and warrants (eventual) acceptance.         
}

\textbf{Response} Thanks for the positive comments.


\rcomment{
Problem complexity. Though the exponential lower bound works, the given example seems excessively degenerate. It works only by assuming that all rectangles have identical area---otherwise, the ``smallest area must aggregate'' severely limits the options. Can the construction be altered to show an exponential lower bound where all regions have unique sizes? (or under some other assumption that is more realistic).

Complementing this paragraph, I'm also wondering if anything is known about whether the specific problem considered here is NP-hard?  
}

\textbf{Response}
We have improved our proof of the exponential lower bound.
In the new proof, all the rectangles have unique sizes;
see Section 3.3 (Exponential Lower Bound).
We have added a section to show that our problem is NP-Hard;
see Section 5 (NP-Hardness Proof).



\rcomment{
Compactness. 
Two models for compactness are given, one being linear and the other not. 
The motivation for needing the linear model is clear, 
but how different are these models? 
Is there any literature that already describes the correlation between these? 
Otherwise, I feel that the paper could be enriched 
by considering the correlation between the two in this setting 
(how well does \Astar-$g_2$ perform in terms of \Astar-$g_1$ / 
does it give similar quality when measuring $g_2$-results using $g_1$, 
or something like that). 
(Assuming you have the data/solutions, this would be comparatively easy to do?)
}

\textbf{Response}
In theory, a patch, $u$, with a small perimeter can be extremely non-compact according to measure~$c(u)$ in \eq3, thus the two measures that we consider are not interchangeable. However, if we assume that all areas of the map have the same size (i.e., $A_u$ is a constant), it would make no difference whether we minimize an area's perimeter or maximize the area's compactness~$c(u)$. Obviously, the areas in our data set have different sizes. However, since we iteratively remove the smallest area, the differences do not become too large. Therefore, measuring the overall compactness of a map based on the total length of all the interior boundaries is quite reasonable.
We have added this argument into Section~4.3 (Cost of Length).

%To the best of our knowledge, 
%there is no literature describing 
%the correlation between the two models for compactness.
%\textcite{Li2013Compactness} reviewed some measures for compactness.
%
%This comment will lead to an interesting research.
%Of course, it is easy to compute the~$g_1$ value of the result of~\Astar-$g_2$,
%and vice versa, since we have the data.
%If we present the values, then it would be better to also theoretically analyze the correlation between the two compactness formulas,
%which could lead to a new paper.
%We may perform this research in our future work.



\rcomment{
Greedy algorithm. 
I'm missing some motivation in the paper for the given greedy choice. 
Specifically, why was this method chosen over other clear candidates? 
(e.g., always ensuring the smallest region is recolored, 
regardless of what the target eventually is?). 
Fig 18. describes a ``bad choice'' for the given greedy strategy, 
but I see no motivation/example of why it would be a good choice.
}

\textbf{Response} 
A motivation for our greedy algorithm is that 
a very similar iterative algorithm has been used by
\textcite{vanOosterom1995Development} 
for constructing the tGAP data structure.
However, we have to make minor modifications to ensure that 
the computed aggregation sequence ends with the goal map that, 
in our situation, is given as a part of the input.
We use our greedy algorithm as a benchmark
so that we are able to see 
whether the \Astar algorithm or the ILP-based algorithm 
indeed perform better.
We have added this argument into Section 6 (A Greedy Algorithm).

We have a start map and a goal map.
Our greedy algorithm must ensure that 
our final result (e.g., the polygon of layer $L_4$ in
Figure 3) will have the type of $T_\mathrm{goal}$.
This is why we should not always recolor the smallest region.
We have added this argument into Section 6 (A Greedy Algorithm).

We use a start map and a goal map because 
our aggregation sequence should be able to 
benefit from a goal map with high quality.
We have added this argument 
into ``Problem definition'' of Section 1 (Introduction).

We have added more details 
into the caption of Fig 18 (Fig 17 in the new version).

\rcomment{
Though I would rate the organization as ``acceptable'', there are many parts where I feel the presentation can be streamlined. Some points of attention:

The contributions only appear on page 5. Much of the text before is ``related work'', but much of it is only tangentially related (e.g. the apparent need to defend optimization approaches)

The problem definition is partially in the preliminaries and in the introduction (though the paragraph ``problem definition'' in the introduction seems to only partially define that problem and then continue with related work).

(Partial) suggestion: restructure such that problem definition and contributions come before ``optimization in map generalization'' and structure the paragraphs of ``optimization in...'' as related work). 
}

\textbf{Response}
We have put ``Problem definition'' and ``Contribution'' 
into Section 1 (Introduction).
We have made an individual section for related work;
see Section 2 (Related Work).


\rcomment{
Personally, I found there to be little value in the 2-page explanation on what linear programming is. It seems out of place to me, and a good place to streamline(/shorten) the exposition.

Variables, objective function and constraints are explained in separate paragraphs. I'm wondering if there is a different organization possible that requires less of the ``recall that ...'' phrasing. For example, some of the basic constraints that relate the meaning of the various variables could just be given immediately.

But that will allow integration of considerations and solutions. For example, when reading the definition of~$c_{t,p,o,r}$, I immediate wonder why that variable helps to solve connectivity (why can't two neighboring areas identify each other as the neighbor...)

It is a style-preference I suppose, but I'd rather have the ILP be built up of variables+constraints+explanation of the various smaller parts, rather than having all the variables up front (and needing to recall).

Also, something that would help a lot here is to use more meaningful variable names (instead of $x,y,z, \ldots$), such that it is easier to remember what's what.

}

\textbf{Response}
Some researchers from Geographic Information Science 
may not be familiar with integer linear programming.
Therefore, we use the coffee example to illustrate the ideas of LP and ILP.
We have put the method of using integer linear programming into Appendix.

We think it is easier for readers 
to find the definitions of variables, objectives, and constraints
if we present them in separate paragraphs.
If we mix the definitions, readers may need to search for a specific definition 
in more paragraphs as some constraints consist of two kinds of variables.

We would also like to use more meaningful variable names, 
but we do not have a good idea to do so as our variables are quite complex.


\rcomment{
With the experimental results, many of the figures and tables are far away from their referencing text. I would suggest introducing some clearpage commands to get them closer to each other, and(/or) adding some more detail to the captions to explain the main observations in the caption as well.
}

\textbf{Response} We have added a clearpage before 
Section 9.2 (Using costs of type change and length).
We have added some details to the captions of 
Figure 17, Figure 23, and Table 1 (see the new version of our paper).




\rcomment{
Introduction: to what degree is continuous generalization, 
in terms of having no discrete changes, 
possible / desirable / realistic?
}

\textbf{Response} 
We consider CMG as the process of producing maps 
covering a continuous interval of scales,
where CMG considers constraints and quality measures 
that act across different scales. 
In our work, for example, the quality of an aggregation sequence 
is measured as a whole; 
whereas in classical generalization approaches, 
only the quality of a map at some output scale is measured.
We consider CMG possible, desirable, and realistic.
In CMG, it is not necessary that 
all changes that occur during zooming are animated in a continuous fashion. 
That fashion may generally be possible. 
A simple way is to fade in and fade out raster maps
as did by \textcite{Pantazis2009b}, 
but it is not at all clear 
whether such animations would improve the experience of the map user.
By contrast, we would like to carry out the changes in a more elegant way;
we would like to change map features gradually.
We have added more arguments in Section 1 (Introduction).


\rcomment{Fig. 5 (and associated text) describes four alternative aggregation preferences. It seems that (unless degenerate) options (b) and (c, d, e) cannot be applicable simultaneously.
}

\textbf{Response} Figure 5 shows some ideas of \textcite{Cheng2006}.
We agree with the reviewer that
options (b) and (c, d, e) cannot be applicable simultaneously.
Still, we think that the figure is useful to exemplify possible choices.
We have added the reference of \textcite{Cheng2006} into the caption.


\rcomment{Section 3.1. Is there a specific reason why the metric considers only the change between two levels? It seems that, unless the resulting patch is immediately part of another aggregation, the choice made has a longer lasting effect. In other words, in all subsequent maps, the choice causes some ``error'' in the intermediate map, yet this is only measured once.
}

\textbf{Response}
A reason is that we want to make the changes small 
so that users can follow maps easily.
We have added this argument in Section 1 (Introduction).


\rcomment{Fig. 24 -- the problem is attributed to the small shared boundary. But is this not ``simply'' an issue of compactness? (That is, the compactness is not weighted heavily enough / or not captured adequately?).
}
\textbf{Response} 
This is an interesting question.
According to our experiences, 
the weight factor that we applied defines a reasonable trade-off 
between the different conflicting objectives, 
when considering a solution as a whole. 
However, we are far from claiming that 
the applied weight factor has been optimally chosen. 
This would probably require a user study, 
which would be beyond the scope of our article.


\rcomment{Can this approach also work if no target map is not given (or has no predefined type)?
}

\textbf{Response} No, we need a target map in order to use the \Astar algorithm,
which finds a shortest path from a start to a target.
However, if there is no target map, we can always generate one 
by an existing algorithm \parencite[e.g.][]{HaunertWolff2010AreaAgg};
see Section 1.


\rcomment{p2: "at any different scales" \\
p3: "as good as" -> "as well as" \\
p3: "He repeatedly displace the line until finding..." -> 
displaced/displaces \& until a ... is found  \\
p4: "based on constr. DT" -> "based on a constr. DT" or 
"based on constr. DTs". \\
p8: "by human being" -> "by a human being" \\
p32: should the equation here not have $p$ and $q$ swapped on one of the sides? \\
}

\textbf{Response} We have improved our paper according to all the six comments.

\clearpage

\section{Reviewer 2}

\rcommentnoskip{
This paper introduces a problem of continuous map generalization in which a set of polygons (referred to as the start map) is sequentially merged into a smaller set of polygons (referred to as the goal map) and compare different approaches in terms of effectiveness and efficiency. Any aggregation sequence must satisfy specific constraints, and of all such sequences, one that minimizes a specific objective function is considered as optimal.

The paper is well written, but there may be some issues to be addressed before it is further considered for publication.

In particular, the importance of the problem is questionable. The problem relies on the following assumptions: 1) The start map and the goal map consist of polygons each of which belongs to a given land-cover type. 2) Every polygon on the goal map is the union of a set of polygons on the start map. The first assumption is fine, but the second assumption considerably limits the problem’s applicability. It seems to me that the problem is interesting if the two maps come from different sources. If the goal map has been derived from the start map, the problem won’t be too different from other problems of continuous map generalization available in the literature.
}

\textbf{Response}
Even if the goal map has been derived from the start map,
the problem is still difficult if we want to have an optimal aggregation sequence.
We have added a section to show that our problem is NP-Hard;
see Section 5 (NP-Hardness Proof).
No literature has tried to solve this problem.

If the two maps come from different sources, one could compute a map overlay and use the result (with combined boundaries from both input maps and land cover classes from the given large-scale map) as the start map.
We have added this argument into ``Problem definition''
of Section 1 (Introduction).


\rcomment{In ``A model could be to minimize the smallest change over all the steps'' (page 3) doesn't sound right. Do you mean to minimize ``greatest'' change? I may overlook something, though.
}

\textbf{Response} We indeed meant that 
an idea could be to minimize the ``smallest'' change
because we wanted to show that ideas could be bad.
However, in the new version of our paper, we changed ``smallest''
to ``greatest'' because using ``smallest'' sounds too stupid.

\rcomment{
The detailed introduction of ``Integer Linear Programming'' (with a coffee example) is not necessary.
}

\textbf{Response}
Some researchers from Geographic Information Science 
may not be familiar with integer linear programming.
Therefore, we use the coffee example to illustrate the ideas of LP and ILP.
We have put the method of using integer linear programming into Appendix.


\rcomment{The discussion on the relevance of compactness to the problem is weak. Should we impose the compactness criterion on all polygons? Some landscape features may inherently take linear forms, e.g., corridors. Does the measurement presented in Sec. 3.2 take into account the irregularity of polygons’ shapes (especially on the start map)? Is it possible (or likely or unlikely) that a non-compact polygon to be aggregated causes any bias in the aggregation sequence? 
}

\textbf{Response}
Indeed, our method will be more reasonable 
by taking into account irregular shapes.
This extension can be a future work of our paper.
In Section 10 (Concluding Remarks),
we have added
``In our setting, we ignored the fact that
some features may inherently take linear forms
(e.g., rivers)''.

\clearpage

\section{Reviewer 3}

\rcommentnoskip{

The introduction is very clear, and motivates the reader to read further. 

I'm usually not a fan of having the related work woven into the introduction, but in this case, the presentation remains clear, the proposed problem is well-defined, and the contributions are delineated from the related work. In particular, the examples of Figure 1 and Figure 2 take the reader by the hand to help them understand the problem. Specifically, the authors nicely clarify that both the ``start map'' and the ``goal map'' are BOTH input parameters.
 
This clarifies that the goal of this paper is not to find a good generalized map, but rather, how to smoothly generate it.

Overall, the introduction is a pleasure to read. 
}

\textbf{Response} We have put the related work in an individual section;
see Section 2 (Related Work).



\rcomment{Major Revision 1: 

Your model is clear and well-described. The presented subdivision graph is clearly understood.

I understand that you are using shortest-path algorithms such as A* on this graph. However, this graph is a directed acyclic graph (DAG), for which more efficient solutions for shortest-path search are known, such as ones based on topological sorting. It appears to me, that the DAG property of the subdivision graph can be exploited to achieve higher efficiency. 

Baseline algorithms used for the experimental evaluation should include shortest path algorithms which are designed for DAGs.
}

\textbf{Response} We have added a section to show that our problem is NP-Hard;
see Section 5 (NP-Hardness Proof).

An off-the-shelf shortest-path algorithm for directed acyclic graphs
(e.g., \textcite[\sect25.2]{Cormen2009})
will explore the whole graph, which has exponential size.
Therefore, it is almost impossible to use a DAG-based algorithm
for our problem.
We have added this argument in Section 3.4 (Methods).


\rcomment{Major Revision 2:

The exponential lower bound in Section 2  appears to be wrong. It shows that your solution has an exponential lower bound, but it does not show that the problem has an exponential lower bound. 

This is similar to the problem of sorting at array of~$n$ numerical values: There is an exponential number ($n!$) of possible intermediate steps. Any approach that enumerates all of these $n!$ possible orderings is exponential. Yet, the problem of sorting an array is not exponential.

The same appears to apply here: Would it be possible to simply aggregate the smallest region in each step, yielding a total run-time of $n$? 

Would that solution be optimal? That would depend on your cost function, which is not yet defined in this part of the paper. 

But in the preliminaries, you state that: ``To find a sequence of small changes that transforms the start map into the goal map, we require that every change involves only two areas of the current map. More precisely, in each step the smallest area~$u$ is merged with one of its neighbors (adjacent areas) $v$ such that~$u$ and~$v$ are replaced by their union.''

This implies that a greedy approach that always takes the smallest values would be optimal, thus leading to the ``merge-sort'' like approach sketched earlier in the paper.
 
Your proof of an exponential lower bound must show, not only that there exists and exponential number of states, but that any correct algorithm must explore an exponential number of states in the worst-case. Or, in other words, in the worst-case, an exponential number of nodes cannot be pruned. 
But if the task is simply to merge the smallest area with the smallest adjacent areas, then only one option remains unpruned in each step, yielding linear run-time.

Note that I am not saying that the problem is not exponentially hard for certain cost functions. But you need to clarify, for the cost function that you propose, whether the problem is exponentially hard, and how greedy and dynamic programming algorithms can not possibly find the correct answer, efficiently.
}

\textbf{Response}
We have improved our illustration of exponential lower bound;
see Section 3.3 (Exponential Lower Bound).
We have added a section to show that our problem is NP-Hard;
see Section 5 (NP-Hardness Proof).


\rcomment{

I'm confused about the combination of cost functions. You propose three different cost functions, based on type ($g_\mathrm{type}$), based on compactness ($g_\mathrm{comp}$), and based on length ($g_\mathrm{lgth}$). However, in Section 3.4., you only ``combine the two cost functions''. 

Why not combine all three? Why is the third defined? Why are there two combinations using two different two-subsets of cost functions?
}

\textbf{Response}
As illustrated in Section~4 (Cost Functions),
our cost function takes two aspects into account: 
one based on semantics and the other based on shape.
This is why we define~$g_\mathrm{type}$ and~$g_\mathrm{comp}$.
We also want to compare the \Astar and a method based on integer linear programming
in solving our problem, while $g_\mathrm{comp}$ cannot be computed linearly.
So we combine~$g_\mathrm{type}$ and~$g_\mathrm{lgth}$ when we make the comparison.
We have made this more clear in Section~4.4 (Combining Cost Functions). 


\rcomment{
Major Revision 3: 

Section 4 is incomplete. This section defines a first part of a greedy baseline algorithm, and defines two cost functions. However, it is not clear how a greedy algorithm would utilize these cost functions. 

A greedy algorithm should simply choose the patch which minimizes 
$argmax_\mathrm{candidate~patches}$ CostFunctionDefeindInSection3.

The authors need to properly define the Greedy algorithm. Reading the experiments, I'm merely assuming that the greedy algorithm does what I expect it to do (see above).
}

\textbf{Response} We have improved the section,
which becomes Section 6 (A Greedy Algorithm) in the new version of our paper.


\rcomment{
Very Major Problem 1: 

The proposed Integer Linear Programming algorithm does not have convincing results. It only terminates for goal polygons having a trivially small number of sub-polygons. Even in the cases where it terminates, the result is worse than A*, and not significantly better than the naive greedy. 

Due to these terrible results, I feel that there is not much intellectual merit in this approach. I suggest that authors remove details of this approach, almost entirely. The authors could still use this approach as a competitor (explained in one paragraph) in the experiments. But as a main contribution, this approach fails to deliver any meaningful results. 

Instead, the authors may try different approaches that exploit that the subdivision graph is a DAG. Exploiting this property may allow significant improvement over A*.
}

\textbf{Response}
We have put the method of using integer linear programming into Appendix.

We have added a section to show that our problem is NP-Hard;
see Section 5 (NP-Hardness Proof).

An off-the-shelf shortest-path algorithm for directed acyclic graphs
(e.g., \textcite[\sect25.2]{Cormen2009})
will explore the whole graph, which has exponential size.
Therefore, it is almost impossible to use a DAG-based algorithm
for our problem.
We have added this argument in Section 3.4 (Methods).



\rcomment{Minor comments: 

Page 6 Line 45: ``The patch also has the property type''. 
Please clarify what this means. I am assuming, that you want to say that each patch also has a unique land-cover area type.
In Equation 1: Region~$R$ is undefined? Should it be~$v$ instead of~$R$?

I cannot understand the idea of Figure 9. It appears that Figure 9a is not a valid partitioning.
}

\textbf{Response} According to your suggestion, 
we changed ``The patch also has the property type''
to ``Each patch also has a unique land-cover type''.
In the new version, we define region~$R$ in Section 3 (Preliminaries).

Figure 9a is not a valid partition, but it is fine for estimation.
In the new version of our paper, we add a reference to Section 7.1.


\rcomment{To summarize: 

I like the problem defined in this paper, and I like the motivation and presentation. However, the main approach presented in this work (using Linear Programming) does not yield any useful theoretic or experimental results. Thus, the intellectual merit of this approach is highly questionable. 

If the authors could find a better performing solution, that has more technical contribution than the \Astar and Greedy approaches, this could be a great paper!

Overall, I feel that this is more of a ``Submit as New'' recommendation from my side. But since I have to decide between Major Revision and Reject, I chose the former.
}

\textbf{Response} Formalizing our problem as a path finding and solving it with the \Astar are two of our main contributions.
We do not have a better solution so far.
We have put the method of using integer linear programming into Appendix.

\clearpage

\section{Reviewer 4}

\rcommentnoskip{
The paper deals with a very common problem when drawing a map at different scales. Typically, when drawing a `zoomed-out' view of a map, not all details need to or in fact should be drawn to avoid cluttering of the visualization. How to deal with unwanted details depends on the nature of the respective details, for roads, one typically just drops smaller and unimportant roads while zooming out. The focus of this paper is how to deal with areas while zooming out -- here simply dropping them is typically not what is desired, e.g., if we have a city partitioned in many quarters, from some distance these quarters should not simply disappear but rather be merged to represent the area of the city as a whole. 
 
The authors focus on sequential merges, that is, always two areas are aggregated in a single step until a target subdivision is reached. Not surprisingly there are exponentially many ways of reaching the target subdivision. By associating costs to each merge step (based on type changes, compactness and boundary lengths), they can evaluate any merge sequence. While of course aiming for an optimal solution they first propose a simple greedy algorithm. Then an A* based approach is presented which in principle can find the optimum solution, yet typically runs out of memory, so a variant which cannot guarantee to find the optimum is employed. Finally an ILP formulation is presented. The approaches are evaluated in the experimental section. 
}

\textbf{Response} Thanks for the summary.


\rcomment{
[Strengths]
The paper is very nicely written, the reader is cautiously introduced to all concepts. The problem as such is interesting and important. 
}

\textbf{Response} Thanks for the positive comments.


\rcomment{
[Weaknesses]
What is the conceptual difference to [25]? This seems very much related with the main difference that the current work also cares about the quality of intermediate aggregations.
Somehow I found the experimental evaluation somewhat underwhelming. The gain from running A* which is about two orders of magnitudes slower seems miniscule to me. Likewise the application of an ILP solver seems overkill. In fact I do not have a good feeling for whether a slightly better solution via ILP or A* really makes a difference for me when I see the respective intermediate maps.   

While I think it is a nice paper, I miss a crip or surprising insight or algorithmic technique. In fact in particular in the light of [25] it seems as a somewhat straightforward extension. Also, I suspect that whether the final results are pleasing to the eye or not, is more dependent on the choice of the cost functions rather than the optimal or close-to-optimal computation of a merge sequence. 

The paper should certainly be published, but for a flagship journal like TSAS I do not think that the paper is strong enough.
}

\textbf{Response} 
\textcite{HaunertWolff2010AreaAgg}, 
reference 25 of the previously submitted version, 
deals with map generalization,
while our method deals with \emph{continuous} map generalization.
Even the results of the \Astar algorithm are only slightly better
than that of the greedy algorithm, 
it is still worth to use the results of the \Astar algorithm.
We can save the results obtained by the \Astar algorithm on a server.
When users are zooming on a map, we can simply send the precomputed results.

It is true that whether the final results are pleasing to the eye or not 
is also dependent on the choice of the cost, 
but it is still important to get optimum results according to a certain cost.
Even using a perfect cost function, a greedy algorithm can return a bad result.
If we get a bad result from a greedy algorithm,
then it is difficult to tell if the bad result is from the choice of the cost
or from the greedy algorithm.
We have explained this more clearly in Section 2.2 
(Optimization in Map Generalization).


%
%\rcomment{
%Generally, the paper would benefit from a bit of reorganization.
%}
%
%\textbf{Response} 
%We have reorganized our paper somewhat.
%
%\rcomment{
%Your writing is clear and concise (thank you!), but I often found myself having to accept that explanation of some critical component might come later (which it generally does). One example of this is with regard to type-change cost, which is described well with respect to its weighted calculation at a given node along a given path, but itself is never defined until you present a case study with ATKIS data.
%}
%
%\textbf{Response} 
%We have improved our writing according to this comment. Speaking of type-change cost, we think it is better to give the details with ATKIS data because the type-change cost is dependent on the data and known from the input.
%
%\rcomment{
%I found myself wondering what exactly costs were measured by as soon as they were mentioned (i.e., what units are used in the function $\mathrm{Cost}(x,y)$). You explain shape-change costs well (counts of boundaries, etc.), but you only give a measure of type-change costs in your case study.I admit I'm a little surprised at your choice -- ordinal count of sequenced land cover class -- because it is fraught with subjectivity and error, and stands in contrast to the admirable rigor you employ at every other point. Despite a reasonable effort by anyone classifying land cover types, they are certainly not ordinal (except perhaps for small "local" cases like sparse vs. dense forest). That means that the ordinal differences you count in your calculation of type-change cost are meaningless. (a suggestion: type-change cost estimation measured as the area changed (i.e., area made uncertain) at each possible aggregation).
%}
%
%\textbf{Response} 
%We think you misunderstood our definition of $\mathrm{Cost}(x,y)$. In the revised version, we added the tree for type-change cost. Now it is clearer how we define the cost.
%
%
%\rcomment{
%The paper is nicely formal, but my sense is that you're perhaps not accounting for (messy) real-world geography enough. You seem to be very casually ignoring class heterogeneity in your aggregated polygons. (If you develop an appropriate type-change cost metric, you could keep reasonable track of this.) How well does your algorithm handle area-fractured or thin linear areas such as braided streams, highways, canals?
%}
%
%\textbf{Response}
%Indeed, we haven't considered thin linear areas such as braided streams, highways, canals. These thin linear areas had been removed from a earlier work by \citet[Chapter~6]{haunert2008f}.
%
%\rcomment{
%	You mention in the conclusion (and not before!) that one objective is to keep more "important" patches for longer -- yet you do not do this at all here. What you do is cull the smallest patches at each step (i.e., you are using area as the criteria, not importance). Also, twice you make relatively objective assumptions in your algorithm that I don't think you satisfactorily explain, and they need explanation because they're geographic impossibilities: 1) why do you work toward a single classed polygon at $t_\mathrm{goal}$? and 2) in your type-cost estimation for \AstarNoSpace, does allowing impossible mergers as you do make for a realistic cost estimation?
%}
%
%\textbf{Response}
%We have not considered more "important" patches for longer because this consideration needs more effort. We may consider more "important" patches in our future work. We work toward a single classed polygon at $t_\mathrm{goal}$ because there are many algorithms \citep[e.g.,][]{haunertwolff2010b, vanSmaalen2003, Cheng2006} that aggregate some land-cover areas into one single land-cover area. The data and the generalized results of these algorithms can be used as our input. Our estimation for type-change cost is fine with a realistic cost estimation because we may find a shortest path as long as the estimation is bounded from above by the exact cost of a shortest path.
%
%\rcomment{
%The notion of a seed patch never comes up after being introduced.
%}
%
%\textbf{Response} 
%The notion of a seed (core) patch was used in a previous version. Sorry for that we forgot to remove the notion in the submission. Now we have removed it.
%
%
%\rcomment{
%Surely there is more literature on polygon aggregation/segmentation using graph theory approaches in other fields like computer vision? You should review this material.
%}
%
%\textbf{Response}
%We have reviewed more literatures on polygon aggregation; see the section of introduction.	
%
%\rcomment{
%Clarify sooner that the particular neighboring polygon into which the smallest polygon is aggregated is made by a global optimization of cost.
%}
%
%\textbf{Response}
%We have improved our paper according to this comment; see the section of preliminaries.
%
%\rcomment{
%Equation 3: what's $n$ exactly? Number of possible succeeding nodes? Or do you mean the number of nodes in the path?
%}
%
%\textbf{Response}
%Variable $n$ is the number of land-cover areas on the start map. The union of the $n$ areas is the only region on the goal map. We have added the description of $n$ in the section of preliminaries.
%
%\rcomment{
%I would appreciate a better rationale for why you absorb the smallest polygon at each iteration. For example, it would be difficult to make the case that the smallest is the least important (because small things in the real world can be greatly important), but relatively easy to say it's the least visibly-important (as per ideas like Tobler's map resolution, or Li and Openshaw's Natural Principle).
%}
%
%\textbf{Response}
%We consider the smallest area as the least important, instead of involving more rules for importance, to avoid making our problem too complicated. We have added this argument in the section of preliminaries.
%
%\rcomment{
%Be careful to keep a clear terminological distinction between different meanings of "edge" - in 2.4, you're talking about what I would call arcs, or line segments, between vertices in a polyline, whereas "edge" was used earlier to refer to the graph theory concept.
%}
%
%\textbf{Response}
%In the revised version, we use "arc" to refer to the graph theory concept. We use "edge" for the line segments of the patch boundaries. 
%
%
%\rcomment{
%Fig 11 - not useful, even though all the others are. This figure would be better if you gave the same region at the same series of iterations with multiple K values (i.e., isolate the K variable).
%}
%
%\textbf{Response} 
%This figure is actually useful. In our method, we repeat increasing overestimation factor until we have a feasible solution. The results displayed in the figure are from the least repetitions we need in order to have feasible solutions. We have made this idea clearer in the revised version.
%
%\rcomment{
%In the conclusions, you seem certain that too much overestimation of costs led to poor results, whereas earlier in the paper you suggest it could be that, or a poor estimation metric.
%}
%
%\textbf{Response}
%A larger overestimation factor does not necessarily mean a poorer result. A better estimation can help \AstarSpace reduce more search space. We have added these descriptions in the revised version; see the section of formalizing area aggregation as a pathfinding problem and the section of conclusions.
%
%\clearpage
%
%\section{Reviewer 2}
%
%\rcomment{
%The "Model", "Notation", and "Exponential lower bound" are fundamentals of the research. They should be put in the methodology section.
%}
%
%\textbf{Response} 
%We made a new section "Preliminaries", and put "Model", "Notation", and "Exponential lower bound" in the new section.
%
%\rcomment{
%Why use \AstarSpace algorithm? Any other alternative methods?
%}
%
%\textbf{Response} 
%We have compared the \AstarSpace algorithm with the integer linear programming (ILP) algorithm. We have added a sentence about this comparison in the revised version; see the section of conclusions. We should also compare the \AstarSpace algorithm with a greedy algorithm, which is our future work.
%
%\rcomment{
%In the experiment, it should be better if the results can be compared to other methods.
%}
%
%\textbf{Response} 
%We compared the algorithm with the ILPs algorithm, and the \AstarSpace algorithm did a better job. We have added a sentence about this comparison in the revised version; see the section of conclusions.
%
%\rcomment{
%In the paper, too many active voice is used. Sentences with "we want/consider/define..." can be revised to passive voice.
%}
%
%\textbf{Response} 
%We have removed some inappropriate active voice. However, we believe that active voice is more direct and clear, so we often use active voice.
%
%\rcomment{
%In Fig1, "input (goal map)" -> "output (goal map)".
%}
%
%\textbf{Response} 
%In Fig1, "input (goal map)" is correct.	



%\clearpage

%	\section{Reviewer 3}
%	
%	\rcomment
%	{
%		I would like to see a few add ons to substantiate the approach in particular
%		regarding its limits...
%	}
%	
%	\textbf{Response}
%	
%	We want to thank the reviewer for this comment. In order to satisfy this
%	petition we have added a new Subsection...
%	
%	We thank the reviewer for this comment and we agree with him/her. We have
%	rewritten the paragraph following his/her comments: 	
%	
%	\begin{quotation}\noindent
%		Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem
%		Ipsum has been the industry's standard dummy text ever since the 1500s, when an
%		\newtext{unknown printer} took a galley of type and scrambled it to make a type
%		specimen book. It has survived not only five centuries, but also the leap into
%		electronic typesetting, remaining essentially unchanged. It was popularised in
%		the 1960s \newtext{the release of Letraset sheets containing Lorem Ipsum
%			passages}, and more recently with desktop publishing software like Aldus
%		PageMaker including versions of Lorem Ipsum.
%	\end{quotation} 

%	 Uncomment in case references are needed
%\bibliographystyle{apalike}
%\bibliography{References}

\printbibliography
	
\end{document}

